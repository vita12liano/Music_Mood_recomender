{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b1f255",
   "metadata": {},
   "source": [
    "# Cell 1 - Import & path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b580b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1da923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: (169909, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>year</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>macro_cluster</th>\n",
       "      <th>subcluster</th>\n",
       "      <th>subcluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6KbQ3uYMLKb5jDxLF7wYDD</td>\n",
       "      <td>Singende Bataillone 1. Teil</td>\n",
       "      <td>['Carl Woitschach']</td>\n",
       "      <td>0</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>-12.428</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>118.469</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>158648</td>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>Warm Emotional Calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6KuQTIu1KoTTkLXKrwlLPV</td>\n",
       "      <td>Fantasiestücke, Op. 111: Più tosto lento</td>\n",
       "      <td>['Robert Schumann', 'Vladimir Horowitz']</td>\n",
       "      <td>0</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>-28.454</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>83.972</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>282133</td>\n",
       "      <td>1</td>\n",
       "      <td>1_0</td>\n",
       "      <td>Deep Calm &amp; Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6L63VW0PibdM1HDSBoqnoM</td>\n",
       "      <td>Chapter 1.18 - Zamek kaniowski</td>\n",
       "      <td>['Seweryn Goszczyński']</td>\n",
       "      <td>0</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>-19.924</td>\n",
       "      <td>0.9290</td>\n",
       "      <td>107.177</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>104300</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>Short Spoken Calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6M94FkXd15sOAOQYRnWPN8</td>\n",
       "      <td>Bebamos Juntos - Instrumental (Remasterizado)</td>\n",
       "      <td>['Francisco Canaro']</td>\n",
       "      <td>0</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>-14.734</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>108.003</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>180760</td>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>Warm Emotional Calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6N6tiFZ9vLTSOIxkj8qKrd</td>\n",
       "      <td>Polonaise-Fantaisie in A-Flat Major, Op. 61</td>\n",
       "      <td>['Frédéric Chopin', 'Vladimir Horowitz']</td>\n",
       "      <td>1</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>-16.829</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>62.149</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>687733</td>\n",
       "      <td>1</td>\n",
       "      <td>1_0</td>\n",
       "      <td>Deep Calm &amp; Minimal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id                                     track_name  \\\n",
       "0  6KbQ3uYMLKb5jDxLF7wYDD                    Singende Bataillone 1. Teil   \n",
       "1  6KuQTIu1KoTTkLXKrwlLPV       Fantasiestücke, Op. 111: Più tosto lento   \n",
       "2  6L63VW0PibdM1HDSBoqnoM                 Chapter 1.18 - Zamek kaniowski   \n",
       "3  6M94FkXd15sOAOQYRnWPN8  Bebamos Juntos - Instrumental (Remasterizado)   \n",
       "4  6N6tiFZ9vLTSOIxkj8qKrd    Polonaise-Fantaisie in A-Flat Major, Op. 61   \n",
       "\n",
       "                                artist_name  popularity  year  acousticness  \\\n",
       "0                       ['Carl Woitschach']           0  1928         0.995   \n",
       "1  ['Robert Schumann', 'Vladimir Horowitz']           0  1928         0.994   \n",
       "2                   ['Seweryn Goszczyński']           0  1928         0.604   \n",
       "3                      ['Francisco Canaro']           0  1928         0.995   \n",
       "4  ['Frédéric Chopin', 'Vladimir Horowitz']           1  1928         0.990   \n",
       "\n",
       "   danceability  energy  instrumentalness  liveness  loudness  speechiness  \\\n",
       "0         0.708  0.1950             0.563    0.1510   -12.428       0.0506   \n",
       "1         0.379  0.0135             0.901    0.0763   -28.454       0.0462   \n",
       "2         0.749  0.2200             0.000    0.1190   -19.924       0.9290   \n",
       "3         0.781  0.1300             0.887    0.1110   -14.734       0.0926   \n",
       "4         0.210  0.2040             0.908    0.0980   -16.829       0.0424   \n",
       "\n",
       "     tempo  valence  duration_ms  macro_cluster subcluster  \\\n",
       "0  118.469   0.7790       158648              1        1_1   \n",
       "1   83.972   0.0767       282133              1        1_0   \n",
       "2  107.177   0.8800       104300              0        0_0   \n",
       "3  108.003   0.7200       180760              1        1_1   \n",
       "4   62.149   0.0693       687733              1        1_0   \n",
       "\n",
       "      subcluster_label  \n",
       "0  Warm Emotional Calm  \n",
       "1  Deep Calm & Minimal  \n",
       "2    Short Spoken Calm  \n",
       "3  Warm Emotional Calm  \n",
       "4  Deep Calm & Minimal  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PROCESSED_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "data_path = os.path.join(DATA_PROCESSED_DIR, \"spotify_dataset_clustered.csv\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Dataset loaded:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a87982af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Righe con subcluster: (169909, 18)\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"subcluster\"].notna()].copy()\n",
    "print(\"Righe con subcluster:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a0190",
   "metadata": {},
   "source": [
    "# Cell 2 - Selecting Audio features from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e9ffb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di subcluster (classi): 11\n",
      "Classi: ['0_0' '0_1' '1_0' '1_1' '1_2' '2_0' '2_1' '2_2' '2_3' '2_4' '2_5']\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \"acousticness\",\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"loudness\",\n",
    "    \"speechiness\",\n",
    "    \"tempo\",\n",
    "    \"valence\",\n",
    "    \"duration_ms\",\n",
    "]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "\n",
    "# Target: subcluster (es. \"2_5\")\n",
    "y_str = df[\"subcluster\"].astype(str).values\n",
    "\n",
    "# Encodiamo i subcluster in interi 0..K-1\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_str)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Numero di subcluster (classi):\", num_classes)\n",
    "print(\"Classi:\", le.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33147cd1",
   "metadata": {},
   "source": [
    "# Cell 3 - Train/val/test split + scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dea558c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (135927, 10) Val: (16991, 10) Test: (16991, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b25ad",
   "metadata": {},
   "source": [
    "# Cell 4 - Dataset & DataLoader PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14bf908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpotifyClusterDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)   # long per CrossEntropy\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SpotifyClusterDataset(X_train_scaled, y_train)\n",
    "val_ds   = SpotifyClusterDataset(X_val_scaled, y_val)\n",
    "test_ds  = SpotifyClusterDataset(X_test_scaled, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8b9c4",
   "metadata": {},
   "source": [
    "# Cell 5 - Definition of the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a521117",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(feature_cols)\n",
    "hidden_dim = 64   # o 128, come avevi prima\n",
    "\n",
    "class MLPCluster(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)  # <-- K classi\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = MLPCluster(input_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99a545",
   "metadata": {},
   "source": [
    "# Cell 6 - train/val per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f426dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | train loss: 0.1732, acc: 0.933 | val loss: 0.1274, acc: 0.952\n",
      "Epoch 2/20 | train loss: 0.1697, acc: 0.934 | val loss: 0.1276, acc: 0.951\n",
      "Epoch 3/20 | train loss: 0.1696, acc: 0.934 | val loss: 0.1267, acc: 0.951\n",
      "Epoch 4/20 | train loss: 0.1704, acc: 0.934 | val loss: 0.1255, acc: 0.951\n",
      "Epoch 5/20 | train loss: 0.1697, acc: 0.933 | val loss: 0.1259, acc: 0.951\n",
      "Epoch 6/20 | train loss: 0.1688, acc: 0.934 | val loss: 0.1244, acc: 0.951\n",
      "Epoch 7/20 | train loss: 0.1669, acc: 0.935 | val loss: 0.1255, acc: 0.951\n",
      "Epoch 8/20 | train loss: 0.1674, acc: 0.935 | val loss: 0.1247, acc: 0.952\n",
      "Epoch 9/20 | train loss: 0.1663, acc: 0.936 | val loss: 0.1256, acc: 0.950\n",
      "Epoch 10/20 | train loss: 0.1648, acc: 0.936 | val loss: 0.1228, acc: 0.951\n",
      "Epoch 11/20 | train loss: 0.1653, acc: 0.936 | val loss: 0.1237, acc: 0.952\n",
      "Epoch 12/20 | train loss: 0.1655, acc: 0.935 | val loss: 0.1229, acc: 0.952\n",
      "Epoch 13/20 | train loss: 0.1640, acc: 0.936 | val loss: 0.1230, acc: 0.953\n",
      "Epoch 14/20 | train loss: 0.1629, acc: 0.936 | val loss: 0.1226, acc: 0.950\n",
      "Epoch 15/20 | train loss: 0.1636, acc: 0.936 | val loss: 0.1223, acc: 0.953\n",
      "Epoch 16/20 | train loss: 0.1630, acc: 0.936 | val loss: 0.1235, acc: 0.950\n",
      "Epoch 17/20 | train loss: 0.1623, acc: 0.937 | val loss: 0.1222, acc: 0.953\n",
      "Epoch 18/20 | train loss: 0.1607, acc: 0.937 | val loss: 0.1211, acc: 0.952\n",
      "Epoch 19/20 | train loss: 0.1614, acc: 0.937 | val loss: 0.1215, acc: 0.952\n",
      "Epoch 20/20 | train loss: 0.1591, acc: 0.938 | val loss: 0.1209, acc: 0.952\n",
      "Loaded best model: val_loss=0.1209, val_acc=0.952\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(loader, model, criterion, optimizer=None):\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * y_batch.size(0)\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = run_epoch(train_loader, model, criterion, optimizer)\n",
    "    val_loss, val_acc = run_epoch(val_loader, model, criterion, optimizer=None)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "        f\"train loss: {train_loss:.4f}, acc: {train_acc:.3f} | \"\n",
    "        f\"val loss: {val_loss:.4f}, acc: {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    # ✅ aggiorna il best model (qui uso val_loss come criterio principale;\n",
    "    # in caso di pareggio, scelgo quello con val_acc migliore)\n",
    "    if (val_loss < best_val_loss) or (\n",
    "        np.isclose(val_loss, best_val_loss) and val_acc > best_val_acc\n",
    "    ):\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "# ✅ alla fine del training, ricarichiamo i pesi migliori trovati\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    print(\n",
    "        f\"Loaded best model: val_loss={best_val_loss:.4f}, \"\n",
    "        f\"val_acc={best_val_acc:.3f}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: nessun best_state_dict salvato (controlla il loop di training).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1a5a2",
   "metadata": {},
   "source": [
    "# Cell 7 - Validation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67363c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9501500794538285\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_0       0.98      0.98      0.98       258\n",
      "         0_1       0.92      0.95      0.94       129\n",
      "         1_0       0.95      0.96      0.96      1337\n",
      "         1_1       0.96      0.94      0.95      1278\n",
      "         1_2       0.94      0.95      0.95       717\n",
      "         2_0       0.95      0.93      0.94      2190\n",
      "         2_1       0.94      0.96      0.95      2714\n",
      "         2_2       0.96      0.96      0.96      3470\n",
      "         2_3       0.95      0.96      0.96      1962\n",
      "         2_4       0.93      0.92      0.92      1974\n",
      "         2_5       0.95      0.97      0.96       962\n",
      "\n",
      "    accuracy                           0.95     16991\n",
      "   macro avg       0.95      0.95      0.95     16991\n",
      "weighted avg       0.95      0.95      0.95     16991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        logits = model(X_batch)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_true.append(y_batch.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_true = np.concatenate(all_true)\n",
    "\n",
    "print(\"Test accuracy:\", (all_preds == all_true).mean())\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(all_true, all_preds, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e398e",
   "metadata": {},
   "source": [
    "# Cell 8 - Saving Model & Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0e8517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join(\"..\", \"models\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"mlp_subcluster.pth\"))\n",
    "np.save(os.path.join(MODEL_DIR, \"scaler_mean.npy\"), scaler.mean_)\n",
    "np.save(os.path.join(MODEL_DIR, \"scaler_scale.npy\"), scaler.scale_)\n",
    "np.save(os.path.join(MODEL_DIR, \"label_encoder_classes.npy\"), le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf368458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | train loss: 0.5928, acc: 0.804 | val loss: 0.2279, acc: 0.917\n",
      "Epoch 2/20 | train loss: 0.2929, acc: 0.887 | val loss: 0.1929, acc: 0.928\n",
      "Epoch 3/20 | train loss: 0.2603, acc: 0.900 | val loss: 0.1790, acc: 0.932\n",
      "Epoch 4/20 | train loss: 0.2403, acc: 0.907 | val loss: 0.1660, acc: 0.938\n",
      "Epoch 5/20 | train loss: 0.2249, acc: 0.913 | val loss: 0.1580, acc: 0.936\n",
      "Epoch 6/20 | train loss: 0.2177, acc: 0.915 | val loss: 0.1524, acc: 0.941\n",
      "Epoch 7/20 | train loss: 0.2091, acc: 0.919 | val loss: 0.1474, acc: 0.943\n",
      "Epoch 8/20 | train loss: 0.2032, acc: 0.921 | val loss: 0.1417, acc: 0.945\n",
      "Epoch 9/20 | train loss: 0.1974, acc: 0.923 | val loss: 0.1419, acc: 0.945\n",
      "Epoch 10/20 | train loss: 0.1948, acc: 0.925 | val loss: 0.1395, acc: 0.947\n",
      "Epoch 11/20 | train loss: 0.1927, acc: 0.925 | val loss: 0.1370, acc: 0.948\n",
      "Epoch 12/20 | train loss: 0.1887, acc: 0.926 | val loss: 0.1358, acc: 0.947\n",
      "Epoch 13/20 | train loss: 0.1854, acc: 0.927 | val loss: 0.1343, acc: 0.948\n",
      "Epoch 14/20 | train loss: 0.1838, acc: 0.928 | val loss: 0.1326, acc: 0.948\n",
      "Epoch 15/20 | train loss: 0.1818, acc: 0.929 | val loss: 0.1319, acc: 0.950\n",
      "Epoch 16/20 | train loss: 0.1813, acc: 0.929 | val loss: 0.1325, acc: 0.949\n",
      "Epoch 17/20 | train loss: 0.1769, acc: 0.931 | val loss: 0.1295, acc: 0.949\n",
      "Epoch 18/20 | train loss: 0.1759, acc: 0.932 | val loss: 0.1290, acc: 0.951\n",
      "Epoch 19/20 | train loss: 0.1768, acc: 0.932 | val loss: 0.1283, acc: 0.950\n",
      "Epoch 20/20 | train loss: 0.1751, acc: 0.931 | val loss: 0.1277, acc: 0.950\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(loader, model, criterion, optimizer=None):\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * y_batch.size(0)\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = run_epoch(train_loader, model, criterion, optimizer)\n",
    "    val_loss, val_acc = run_epoch(val_loader, model, criterion, optimizer=None)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "        f\"train loss: {train_loss:.4f}, acc: {train_acc:.3f} | \"\n",
    "        f\"val loss: {val_loss:.4f}, acc: {val_acc:.3f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
